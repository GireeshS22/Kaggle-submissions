# -*- coding: utf-8 -*-
"""Transfer learning

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Zr0lJThZ7N_UnSD94x0ZpLqC7GBIocUt
"""

from google.colab import drive
drive.mount('/content/gdrive', force_remount=True)

import pandas as pd

from keras import applications
from keras.layers import Flatten, Dense, Dropout, Activation
from keras.models import Model
from keras.optimizers import SGD, Adam
from sklearn.model_selection import train_test_split
from keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard

import keras.backend as K
#from time_distributed_read_images import generate_arrays_from_file, generate_val_arrays_from_file
from read_images import generate_arrays_from_file, generate_val_arrays_from_file

from config import height, width, train_file, mini_batch_size

model = applications.VGG16(weights = "imagenet", include_top=False, input_shape = (width, height, 3))

for layer in model.layers[:14]:
    layer.trainable = False

model.summary()

x = model.output
x = Flatten()(x)
x = Dense(32, activation="relu")(x)
x = Dropout(0.4)(x)
x = Dense(16, activation="relu")(x)
dense = Dense(5, name = "dense")(x)
y_pred = Activation('softmax', name='label')(dense)

model_final = Model(input = model.input, output = y_pred)
model_final.summary()

# clipnorm seems to speeds up convergence
#opt = SGD(lr=0.02, decay=1e-6, momentum=0.9, nesterov=True, clipnorm=5)
opt = Adam(lr = 0.00005)

# the loss calc occurs elsewhere, so use a dummy lambda func for the loss
model_final.compile(loss="categorical_crossentropy", optimizer=opt, metrics=['accuracy'])

model_final.load_weights("gdrive/My Drive/Colab/Ship/Checkpoints_tl/td.weights.last.hdf5")
print("Weights loaded ***")

data = pd.read_csv(train_file)
training, validation = train_test_split(data, test_size = 0.05)

training = training.reset_index().drop(columns = ["index"])
validation = validation.reset_index().drop(columns = ["index"])

roundofffortraining = (len(training) // mini_batch_size) * mini_batch_size
roundoffforvalidation = (len(validation) // mini_batch_size) * mini_batch_size

training = training[:roundofffortraining]
validation = validation[:roundoffforvalidation]

print("Training on ", str(len(training)), " samples")
print("Validating on ", str(len(validation)), " samples")

print(training.head())

filepath="gdrive/My Drive/Colab/Ship/Checkpoints_tl/weights.{acc:.3f}a-{loss:.3f}l.hdf5"
checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')

checkpoint_all = ModelCheckpoint("gdrive/My Drive/Colab/Ship/Checkpoints_tl/td.weights.last.hdf5", monitor='loss', verbose=1, save_best_only=False)

callbacks_list = [checkpoint, checkpoint_all]

model_final.fit_generator(generator = generate_arrays_from_file(training), 
                    steps_per_epoch=len(training) // mini_batch_size, 
                    epochs=3000, 
                    callbacks=callbacks_list,
                    validation_data = generate_val_arrays_from_file(validation),
                    validation_steps=100,
                    initial_epoch=0
                    )